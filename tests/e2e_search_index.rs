//! E2E integration tests for search/index pipeline.
//!
//! Tests cover:
//! - Full index flow with temp data-dir
//! - Search with JSON output (hits, match_type, aggregations)
//! - Watch-once environment path functionality
//! - Trace/log file capture (no mocks)
//!
//! Part of bead: coding_agent_session_search-0jt (TST.11)

use assert_cmd::cargo::cargo_bin_cmd;
use coding_agent_search::storage::sqlite::SqliteStorage;
use std::fs;
use std::path::Path;
use std::time::Instant;

mod util;
use util::EnvGuard;
use util::e2e_log::{E2eLogger, E2ePerformanceMetrics, E2ePhase};

// =============================================================================
// E2E Logger Support
// =============================================================================

fn e2e_logging_enabled() -> bool {
    std::env::var("E2E_LOG").is_ok()
}

struct PhaseTracker {
    logger: Option<E2eLogger>,
}

impl PhaseTracker {
    fn new() -> Self {
        let logger = if e2e_logging_enabled() {
            E2eLogger::new("rust").ok()
        } else {
            None
        };
        Self { logger }
    }

    fn start(&self, name: &str, description: Option<&str>) -> Instant {
        let phase = E2ePhase {
            name: name.to_string(),
            description: description.map(String::from),
        };
        if let Some(ref lg) = self.logger {
            let _ = lg.phase_start(&phase);
        }
        Instant::now()
    }

    fn end(&self, name: &str, description: Option<&str>, start: Instant) {
        let duration_ms = start.elapsed().as_millis() as u64;
        let phase = E2ePhase {
            name: name.to_string(),
            description: description.map(String::from),
        };
        if let Some(ref lg) = self.logger {
            let _ = lg.phase_end(&phase, duration_ms);
        }
    }

    fn metrics(&self, name: &str, metrics: &E2ePerformanceMetrics) {
        if let Some(ref lg) = self.logger {
            let _ = lg.metrics(name, metrics);
        }
    }

    fn flush(&self) {
        if let Some(ref lg) = self.logger {
            let _ = lg.flush();
        }
    }
}

/// Helper to create Codex session with modern envelope format.
fn make_codex_session(root: &Path, date_path: &str, filename: &str, content: &str, ts: u64) {
    let sessions = root.join(format!("sessions/{date_path}"));
    fs::create_dir_all(&sessions).unwrap();
    let file = sessions.join(filename);
    // Trailing newline is critical for append_codex_session to work correctly
    let sample = format!(
        r#"{{"type": "event_msg", "timestamp": {ts}, "payload": {{"type": "user_message", "message": "{content}"}}}}
{{"type": "response_item", "timestamp": {}, "payload": {{"role": "assistant", "content": "{content}_response"}}}}
"#,
        ts + 1000
    );
    fs::write(file, sample).unwrap();
}

/// Helper to create Claude Code session.
fn make_claude_session(root: &Path, project: &str, filename: &str, content: &str, ts: &str) {
    let project_dir = root.join(format!("projects/{project}"));
    fs::create_dir_all(&project_dir).unwrap();
    let file = project_dir.join(filename);
    let sample = format!(
        r#"{{"type": "user", "timestamp": "{ts}", "message": {{"role": "user", "content": "{content}"}}}}
{{"type": "assistant", "timestamp": "{ts}", "message": {{"role": "assistant", "content": "{content}_response"}}}}"#
    );
    fs::write(file, sample).unwrap();
}

/// Append an additional Codex message pair (user + assistant) to an existing rollout file.
fn append_codex_session(file: &Path, content: &str, ts: u64) {
    use std::io::Write;

    let mut f = std::fs::OpenOptions::new()
        .append(true)
        .open(file)
        .expect("open rollout for append");
    let sample = format!(
        "{{\"type\": \"event_msg\", \"timestamp\": {ts}, \"payload\": {{\"type\": \"user_message\", \"message\": \"{content}\"}}}}\n{{\"type\": \"response_item\", \"timestamp\": {}, \"payload\": {{\"role\": \"assistant\", \"content\": \"{content}_response\"}}}}\n",
        ts + 1000
    );
    f.write_all(sample.as_bytes()).unwrap();
}

fn count_messages(db_path: &Path) -> i64 {
    let storage = SqliteStorage::open(db_path).expect("open sqlite");
    storage
        .raw()
        .query_row("SELECT COUNT(*) FROM messages", [], |r| r.get(0))
        .expect("count messages")
}

/// Test: Full index pipeline - index --full creates DB and index
#[test]
fn index_full_creates_artifacts() {
    let tracker = PhaseTracker::new();
    let tmp = tempfile::TempDir::new().unwrap();
    let home = tmp.path();
    let codex_home = home.join(".codex");
    let data_dir = home.join("cass_data");
    fs::create_dir_all(&data_dir).unwrap();

    let _guard_home = EnvGuard::set("HOME", home.to_string_lossy());
    let _guard_codex = EnvGuard::set("CODEX_HOME", codex_home.to_string_lossy());

    // Create fixture data
    let phase_start = tracker.start("create_fixtures", Some("Create Codex session fixture"));
    make_codex_session(
        &codex_home,
        "2024/11/20",
        "rollout-1.jsonl",
        "hello world",
        1732118400000,
    );
    tracker.end(
        "create_fixtures",
        Some("Create Codex session fixture"),
        phase_start,
    );

    // Capture memory/IO before indexing (for delta calculation)
    let mem_before = E2ePerformanceMetrics::capture_memory();
    let io_before = E2ePerformanceMetrics::capture_io();

    // Run index --full
    let phase_start = tracker.start("index_full", Some("Execute full index command"));
    cargo_bin_cmd!("cass")
        .args(["index", "--full", "--data-dir"])
        .arg(&data_dir)
        // Avoid connector detection from the repository CWD (e.g. `.aider.chat.history.md`).
        .current_dir(home)
        .env("CODEX_HOME", &codex_home)
        .env("HOME", home)
        .assert()
        .success();
    let index_duration_ms = phase_start.elapsed().as_millis() as u64;
    tracker.end(
        "index_full",
        Some("Execute full index command"),
        phase_start,
    );

    // Capture memory/IO after indexing
    let mem_after = E2ePerformanceMetrics::capture_memory();
    let io_after = E2ePerformanceMetrics::capture_io();

    // Verify artifacts created
    let phase_start = tracker.start("verify_artifacts", Some("Verify database and index exist"));
    assert!(
        data_dir.join("agent_search.db").exists(),
        "SQLite DB should be created"
    );
    assert!(
        data_dir.join("index").exists(),
        "Tantivy index directory should exist"
    );
    tracker.end(
        "verify_artifacts",
        Some("Verify database and index exist"),
        phase_start,
    );

    // Count messages and emit performance metrics
    let msg_count = count_messages(&data_dir.join("agent_search.db")) as u64;
    let mut metrics = E2ePerformanceMetrics::new()
        .with_duration(index_duration_ms)
        .with_throughput(msg_count, index_duration_ms);

    // Add memory delta if available
    if let (Some(before), Some(after)) = (mem_before, mem_after) {
        metrics = metrics.with_memory(after.saturating_sub(before));
    }

    // Add I/O delta if available
    if let (Some((rb, wb)), Some((ra, wa))) = (io_before, io_after) {
        metrics = metrics.with_io(0, 0, ra.saturating_sub(rb), wa.saturating_sub(wb));
    }

    tracker.metrics("index_full", &metrics);
    tracker.flush();
}

/// Incremental re-index must preserve existing messages and ingest new ones from the same file.
#[test]
fn incremental_reindex_preserves_and_appends_messages() {
    let tracker = PhaseTracker::new();
    let tmp = tempfile::TempDir::new().unwrap();
    let home = tmp.path();
    let codex_home = home.join(".codex");
    let data_dir = home.join("cass_data");
    fs::create_dir_all(&data_dir).unwrap();

    let _guard_home = EnvGuard::set("HOME", home.to_string_lossy());
    let _guard_codex = EnvGuard::set("CODEX_HOME", codex_home.to_string_lossy());

    // Initial session
    let phase_start = tracker.start(
        "create_initial_fixture",
        Some("Create initial session with test content"),
    );
    let ts = 1_732_118_400_000u64; // stable timestamp
    make_codex_session(
        &codex_home,
        "2024/11/20",
        "rollout-incremental.jsonl",
        "initial_keep_token",
        ts,
    );
    let session_file = codex_home.join("sessions/2024/11/20/rollout-incremental.jsonl");
    tracker.end(
        "create_initial_fixture",
        Some("Create initial session with test content"),
        phase_start,
    );

    // Full index
    let phase_start = tracker.start("index_full", Some("Run initial full index"));
    cargo_bin_cmd!("cass")
        .args(["index", "--full", "--data-dir"])
        .arg(&data_dir)
        // Avoid connector detection from the repository CWD (e.g. `.aider.chat.history.md`).
        .current_dir(home)
        .env("CODEX_HOME", &codex_home)
        .env("HOME", home)
        .assert()
        .success();
    tracker.end("index_full", Some("Run initial full index"), phase_start);

    // Ensure subsequent writes get a later mtime than the recorded scan start
    std::thread::sleep(std::time::Duration::from_millis(1200));

    // Baseline search should find the initial content
    let phase_start = tracker.start(
        "search_baseline",
        Some("Verify initial content is searchable"),
    );
    let baseline = cargo_bin_cmd!("cass")
        .args(["search", "initial_keep_token", "--robot", "--data-dir"])
        .arg(&data_dir)
        // Avoid connector detection from the repository CWD (e.g. `.aider.chat.history.md`).
        .current_dir(home)
        .env("HOME", home)
        .output()
        .expect("baseline search");
    assert!(baseline.status.success());
    let baseline_json: serde_json::Value =
        serde_json::from_slice(&baseline.stdout).expect("baseline json");
    let baseline_hits = baseline_json
        .get("hits")
        .and_then(|h| h.as_array())
        .map(|v| v.len())
        .unwrap_or(0);
    assert!(baseline_hits >= 1, "initial content should be indexed");
    tracker.end(
        "search_baseline",
        Some("Verify initial content is searchable"),
        phase_start,
    );

    // Append new content to the same file (simulates conversation growth)
    let phase_start = tracker.start(
        "append_content",
        Some("Append new messages to session file"),
    );
    append_codex_session(&session_file, "appended_token_beta", ts + 10_000);
    tracker.end(
        "append_content",
        Some("Append new messages to session file"),
        phase_start,
    );

    // On some filesystems, mtime resolution is 1s; give a small buffer before reindex
    std::thread::sleep(std::time::Duration::from_millis(50));

    // Incremental re-index (no --full)
    let phase_start = tracker.start("index_incremental", Some("Run incremental reindex"));
    cargo_bin_cmd!("cass")
        .args(["index", "--data-dir"])
        .arg(&data_dir)
        // Avoid connector detection from the repository CWD (e.g. `.aider.chat.history.md`).
        .current_dir(home)
        .env("CODEX_HOME", &codex_home)
        .env("HOME", home)
        .assert()
        .success();
    tracker.end(
        "index_incremental",
        Some("Run incremental reindex"),
        phase_start,
    );

    // Original content must still be present
    let phase_start = tracker.start(
        "search_preserved",
        Some("Verify original content preserved"),
    );
    let preserved = cargo_bin_cmd!("cass")
        .args(["search", "initial_keep_token", "--robot", "--data-dir"])
        .arg(&data_dir)
        // Avoid connector detection from the repository CWD (e.g. `.aider.chat.history.md`).
        .current_dir(home)
        .env("HOME", home)
        .output()
        .expect("preserved search");
    assert!(preserved.status.success());
    let preserved_hits = serde_json::from_slice::<serde_json::Value>(&preserved.stdout)
        .unwrap()
        .get("hits")
        .and_then(|h| h.as_array())
        .map(|v| v.len())
        .unwrap_or(0);
    assert!(
        preserved_hits >= baseline_hits,
        "existing messages should not be dropped on reindex"
    );
    tracker.end(
        "search_preserved",
        Some("Verify original content preserved"),
        phase_start,
    );

    // New content must be discoverable
    let phase_start = tracker.start("search_appended", Some("Verify appended content indexed"));
    let appended = cargo_bin_cmd!("cass")
        .args(["search", "appended_token_beta", "--robot", "--data-dir"])
        .arg(&data_dir)
        // Avoid connector detection from the repository CWD (e.g. `.aider.chat.history.md`).
        .current_dir(home)
        .env("HOME", home)
        .output()
        .expect("appended search");
    assert!(appended.status.success());
    let appended_hits = serde_json::from_slice::<serde_json::Value>(&appended.stdout)
        .unwrap()
        .get("hits")
        .and_then(|h| h.as_array())
        .map(|v| v.len())
        .unwrap_or(0);
    assert!(
        appended_hits >= 1,
        "appended content should be indexed during incremental run"
    );
    tracker.end(
        "search_appended",
        Some("Verify appended content indexed"),
        phase_start,
    );

    tracker.flush();
}

/// Reindexing must never drop previously ingested messages in SQLite or Tantivy.
#[test]
fn reindex_does_not_drop_messages_in_db_or_search() {
    let tmp = tempfile::TempDir::new().unwrap();
    let home = tmp.path();
    let codex_home = home.join(".codex");
    let data_dir = home.join("cass_data");
    fs::create_dir_all(&data_dir).unwrap();
    let xdg_data = home.join(".local/share");
    let xdg_config = home.join(".config");
    fs::create_dir_all(&xdg_data).unwrap();
    fs::create_dir_all(&xdg_config).unwrap();

    let _guard_home = EnvGuard::set("HOME", home.to_string_lossy());
    let _guard_codex = EnvGuard::set("CODEX_HOME", codex_home.to_string_lossy());

    // Seed a rollout with two messages
    let ts = 1_732_118_400_000u64;
    make_codex_session(
        &codex_home,
        "2024/11/20",
        "rollout-drop-guard.jsonl",
        "persist_me",
        ts,
    );
    let session_file = codex_home.join("sessions/2024/11/20/rollout-drop-guard.jsonl");

    cargo_bin_cmd!("cass")
        .args(["index", "--full", "--data-dir"])
        .arg(&data_dir)
        // Avoid connector detection from the repository CWD (e.g. `.aider.chat.history.md`).
        .current_dir(home)
        .env("CODEX_HOME", &codex_home)
        .env("HOME", home)
        .env("XDG_DATA_HOME", &xdg_data)
        .env("XDG_CONFIG_HOME", &xdg_config)
        .assert()
        .success();

    // Ensure next write has strictly newer mtime than initial scan start
    std::thread::sleep(std::time::Duration::from_millis(1200));

    let db_path = data_dir.join("agent_search.db");
    let baseline_count = count_messages(&db_path);
    assert_eq!(baseline_count, 2, "initial two messages recorded");

    // Append another turn and reindex incrementally
    append_codex_session(&session_file, "persist_me_again", ts + 5_000);
    std::thread::sleep(std::time::Duration::from_millis(50));
    cargo_bin_cmd!("cass")
        .args(["index", "--data-dir"])
        .arg(&data_dir)
        // Avoid connector detection from the repository CWD (e.g. `.aider.chat.history.md`).
        .current_dir(home)
        .env("CODEX_HOME", &codex_home)
        .env("HOME", home)
        .env("XDG_DATA_HOME", &xdg_data)
        .env("XDG_CONFIG_HOME", &xdg_config)
        .assert()
        .success();

    let after_count = count_messages(&db_path);
    assert_eq!(
        after_count,
        baseline_count + 2,
        "messages should only grow after reindex"
    );

    // Verify both old and new content are searchable (Tantivy layer)
    for term in ["persist_me", "persist_me_again"] {
        let out = cargo_bin_cmd!("cass")
            .args(["search", term, "--robot", "--data-dir"])
            .arg(&data_dir)
            // Avoid connector detection from the repository CWD (e.g. `.aider.chat.history.md`).
            .current_dir(home)
            .env("HOME", home)
            .env("XDG_DATA_HOME", &xdg_data)
            .env("XDG_CONFIG_HOME", &xdg_config)
            .output()
            .expect("search");
        assert!(out.status.success(), "search should succeed for {term}");
        let hits = serde_json::from_slice::<serde_json::Value>(&out.stdout)
            .unwrap()
            .get("hits")
            .and_then(|h| h.as_array())
            .map(|v| v.len())
            .unwrap_or(0);
        assert!(hits >= 1, "{term} should remain indexed");
    }
}

/// Test: Search returns hits with correct match_type
#[test]
fn search_returns_hits_with_match_type() {
    let tmp = tempfile::TempDir::new().unwrap();
    let home = tmp.path();
    let codex_home = home.join(".codex");
    let data_dir = home.join("cass_data");
    fs::create_dir_all(&data_dir).unwrap();

    let _guard_home = EnvGuard::set("HOME", home.to_string_lossy());
    let _guard_codex = EnvGuard::set("CODEX_HOME", codex_home.to_string_lossy());

    // Create fixture with unique content
    make_codex_session(
        &codex_home,
        "2024/11/20",
        "rollout-1.jsonl",
        "unique_search_term_alpha",
        1732118400000,
    );

    // Index first
    cargo_bin_cmd!("cass")
        .args(["index", "--full", "--data-dir"])
        .arg(&data_dir)
        .env("CODEX_HOME", &codex_home)
        .env("HOME", home)
        .assert()
        .success();

    // Search and verify JSON output
    let output = cargo_bin_cmd!("cass")
        .args([
            "search",
            "unique_search_term_alpha",
            "--robot",
            "--data-dir",
        ])
        .arg(&data_dir)
        .env("HOME", home)
        .output()
        .expect("search command");

    assert!(output.status.success(), "Search should succeed");

    let json: serde_json::Value =
        serde_json::from_slice(&output.stdout).expect("valid JSON output");

    // Verify hits array exists
    let hits = json
        .get("hits")
        .and_then(|h| h.as_array())
        .expect("hits array should exist");
    assert!(!hits.is_empty(), "Should find at least one hit");

    // Verify match_type field
    let first_hit = &hits[0];
    assert!(
        first_hit.get("match_type").is_some(),
        "Hit should have match_type field"
    );
    let match_type = first_hit["match_type"].as_str().unwrap();
    assert!(
        ["exact", "prefix", "wildcard", "fuzzy", "wildcard_fallback"].contains(&match_type),
        "match_type should be a known type, got: {}",
        match_type
    );

    // Verify content contains search term
    let content = first_hit["content"].as_str().unwrap_or("");
    assert!(
        content.contains("unique_search_term_alpha"),
        "Content should contain search term"
    );
}

/// Test: Search aggregations include agent buckets
#[test]
fn search_aggregations_include_agents() {
    let tmp = tempfile::TempDir::new().unwrap();
    let home = tmp.path();
    let codex_home = home.join(".codex");
    let claude_home = home.join(".claude");
    let data_dir = home.join("cass_data");
    fs::create_dir_all(&data_dir).unwrap();

    let _guard_home = EnvGuard::set("HOME", home.to_string_lossy());
    let _guard_codex = EnvGuard::set("CODEX_HOME", codex_home.to_string_lossy());

    // Create fixtures from multiple connectors
    make_codex_session(
        &codex_home,
        "2024/11/20",
        "rollout-agg.jsonl",
        "aggregation_test_content",
        1732118400000,
    );
    make_claude_session(
        &claude_home,
        "agg-project",
        "session-agg.jsonl",
        "aggregation_test_content",
        "2024-11-20T10:00:00Z",
    );

    // Index
    cargo_bin_cmd!("cass")
        .args(["index", "--full", "--data-dir"])
        .arg(&data_dir)
        .env("CODEX_HOME", &codex_home)
        .env("HOME", home)
        .assert()
        .success();

    // Search with aggregations
    let output = cargo_bin_cmd!("cass")
        .args([
            "search",
            "aggregation_test_content",
            "--aggregate",
            "agent",
            "--robot",
            "--data-dir",
        ])
        .arg(&data_dir)
        .env("HOME", home)
        .output()
        .expect("search command");

    assert!(output.status.success(), "Search should succeed");

    let json: serde_json::Value = serde_json::from_slice(&output.stdout).expect("valid JSON");

    // Verify aggregations
    let aggregations = json
        .get("aggregations")
        .expect("aggregations field should exist");
    let agent_agg = aggregations.get("agent").expect("agent aggregation");
    let buckets = agent_agg
        .get("buckets")
        .and_then(|b| b.as_array())
        .expect("buckets array");

    let agent_keys: std::collections::HashSet<_> = buckets
        .iter()
        .filter_map(|b| b.get("key").and_then(|k| k.as_str()))
        .collect();

    // At least one of our fixtures should be found in aggregations
    // (Claude works reliably via HOME; Codex via CODEX_HOME may vary by platform)
    assert!(
        agent_keys.contains("codex") || agent_keys.contains("claude_code"),
        "Should include at least one expected agent. Found: {:?}",
        agent_keys
    );
}

/// Test: Watch-once mode indexes specific paths
#[test]
fn watch_once_indexes_specified_path() {
    let tmp = tempfile::TempDir::new().unwrap();
    let home = tmp.path();
    let codex_home = home.join(".codex");
    let data_dir = home.join("cass_data");
    fs::create_dir_all(&data_dir).unwrap();

    let _guard_home = EnvGuard::set("HOME", home.to_string_lossy());
    let _guard_codex = EnvGuard::set("CODEX_HOME", codex_home.to_string_lossy());

    // Create initial data
    make_codex_session(
        &codex_home,
        "2024/11/20",
        "rollout-watch.jsonl",
        "watch_once_initial",
        1732118400000,
    );

    // Initial index
    cargo_bin_cmd!("cass")
        .args(["index", "--full", "--data-dir"])
        .arg(&data_dir)
        .env("CODEX_HOME", &codex_home)
        .env("HOME", home)
        .assert()
        .success();

    // Create new file to watch
    let watch_file = codex_home.join("sessions/2024/11/21/rollout-new.jsonl");
    fs::create_dir_all(watch_file.parent().unwrap()).unwrap();

    // Use current timestamp so message is indexed
    let now_ts = std::time::SystemTime::now()
        .duration_since(std::time::UNIX_EPOCH)
        .unwrap()
        .as_millis() as u64;

    let sample = format!(
        r#"{{"type": "event_msg", "timestamp": {now_ts}, "payload": {{"type": "user_message", "message": "watch_once_new_content"}}}}
{{"type": "response_item", "timestamp": {}, "payload": {{"role": "assistant", "content": "watch_once_response"}}}}"#,
        now_ts + 1000
    );
    fs::write(&watch_file, sample).unwrap();

    // Run watch-once with specific path
    cargo_bin_cmd!("cass")
        .args(["index", "--watch-once"])
        .arg(&watch_file)
        .arg("--data-dir")
        .arg(&data_dir)
        .env("CODEX_HOME", &codex_home)
        .env("HOME", home)
        .assert()
        .success();

    // Verify new content is searchable
    let output = cargo_bin_cmd!("cass")
        .args(["search", "watch_once_new_content", "--robot", "--data-dir"])
        .arg(&data_dir)
        .env("HOME", home)
        .output()
        .expect("search command");

    assert!(output.status.success());
    let json: serde_json::Value = serde_json::from_slice(&output.stdout).expect("valid JSON");
    let hits = json.get("hits").and_then(|h| h.as_array()).expect("hits");
    assert!(
        !hits.is_empty(),
        "Should find the newly indexed watch-once content"
    );
}

/// Test: Search with filters (agent, time range)
#[test]
fn search_with_filters() {
    let tmp = tempfile::TempDir::new().unwrap();
    let home = tmp.path();
    let codex_home = home.join(".codex");
    let data_dir = home.join("cass_data");
    fs::create_dir_all(&data_dir).unwrap();

    let _guard_home = EnvGuard::set("HOME", home.to_string_lossy());
    let _guard_codex = EnvGuard::set("CODEX_HOME", codex_home.to_string_lossy());

    // Create multiple sessions with distinct content
    make_codex_session(
        &codex_home,
        "2024/11/20",
        "rollout-filter1.jsonl",
        "filter_test_content",
        1732118400000, // Nov 20, 2024
    );
    make_codex_session(
        &codex_home,
        "2024/11/21",
        "rollout-filter2.jsonl",
        "filter_test_content",
        1732204800000, // Nov 21, 2024
    );

    // Index
    cargo_bin_cmd!("cass")
        .args(["index", "--full", "--data-dir"])
        .arg(&data_dir)
        .env("CODEX_HOME", &codex_home)
        .env("HOME", home)
        .assert()
        .success();

    // Search with agent filter
    let output = cargo_bin_cmd!("cass")
        .args([
            "search",
            "filter_test_content",
            "--agent",
            "codex",
            "--robot",
            "--data-dir",
        ])
        .arg(&data_dir)
        .env("HOME", home)
        .output()
        .expect("search command");

    assert!(output.status.success());
    let json: serde_json::Value = serde_json::from_slice(&output.stdout).expect("valid JSON");
    let hits = json.get("hits").and_then(|h| h.as_array()).expect("hits");

    // All hits should be from codex agent
    for hit in hits {
        assert_eq!(
            hit["agent"].as_str().unwrap(),
            "codex",
            "All hits should be from codex agent"
        );
    }
}

/// Test: Search returns total_matches and pagination info
#[test]
fn search_returns_pagination_info() {
    let tmp = tempfile::TempDir::new().unwrap();
    let home = tmp.path();
    let codex_home = home.join(".codex");
    let data_dir = home.join("cass_data");
    fs::create_dir_all(&data_dir).unwrap();

    let _guard_home = EnvGuard::set("HOME", home.to_string_lossy());
    let _guard_codex = EnvGuard::set("CODEX_HOME", codex_home.to_string_lossy());

    // Create multiple sessions
    for i in 1..=5 {
        make_codex_session(
            &codex_home,
            "2024/11/20",
            &format!("rollout-page{i}.jsonl"),
            "pagination_test_term",
            1732118400000 + (i as u64 * 1000),
        );
    }

    // Index
    cargo_bin_cmd!("cass")
        .args(["index", "--full", "--data-dir"])
        .arg(&data_dir)
        .env("CODEX_HOME", &codex_home)
        .env("HOME", home)
        .assert()
        .success();

    // Search with limit
    let output = cargo_bin_cmd!("cass")
        .args([
            "search",
            "pagination_test_term",
            "--limit",
            "3",
            "--robot",
            "--data-dir",
        ])
        .arg(&data_dir)
        .env("HOME", home)
        .output()
        .expect("search command");

    assert!(output.status.success());
    let json: serde_json::Value = serde_json::from_slice(&output.stdout).expect("valid JSON");

    // Verify pagination fields
    let total = json
        .get("total_matches")
        .and_then(|t| t.as_u64())
        .expect("total_matches");
    let limit = json.get("limit").and_then(|l| l.as_u64()).expect("limit");
    let hits = json
        .get("hits")
        .and_then(|h| h.as_array())
        .expect("hits")
        .len();

    // We created 5 sessions, each with 2 messages (user + response), so we expect >= 5 hits
    // But some may not match the search term exactly
    assert!(
        total >= 1,
        "Should have at least 1 total match. Got: {}",
        total
    );
    assert_eq!(limit, 3, "Limit should be 3");
    assert!(hits <= 3, "Returned hits should be <= limit");
}

/// Test: Force rebuild recreates index
#[test]
fn force_rebuild_recreates_index() {
    let tmp = tempfile::TempDir::new().unwrap();
    let home = tmp.path();
    let codex_home = home.join(".codex");
    let data_dir = home.join("cass_data");
    fs::create_dir_all(&data_dir).unwrap();

    let _guard_home = EnvGuard::set("HOME", home.to_string_lossy());
    let _guard_codex = EnvGuard::set("CODEX_HOME", codex_home.to_string_lossy());

    // Create initial data
    make_codex_session(
        &codex_home,
        "2024/11/20",
        "rollout-rebuild.jsonl",
        "rebuild_test_initial",
        1732118400000,
    );

    // Initial index
    cargo_bin_cmd!("cass")
        .args(["index", "--full", "--data-dir"])
        .arg(&data_dir)
        .env("CODEX_HOME", &codex_home)
        .env("HOME", home)
        .assert()
        .success();

    // Get initial index file stats
    let index_dir = data_dir.join("index");
    let initial_mtime = fs::metadata(&index_dir).and_then(|m| m.modified()).ok();

    // Wait a bit
    std::thread::sleep(std::time::Duration::from_secs(1));

    // Force rebuild
    cargo_bin_cmd!("cass")
        .args(["index", "--force-rebuild", "--data-dir"])
        .arg(&data_dir)
        .env("CODEX_HOME", &codex_home)
        .env("HOME", home)
        .assert()
        .success();

    // Verify index was rebuilt (mtime changed)
    let new_mtime = fs::metadata(&index_dir).and_then(|m| m.modified()).ok();

    assert!(
        initial_mtime != new_mtime,
        "Index mtime should change after force-rebuild"
    );

    // Verify content is still searchable
    let output = cargo_bin_cmd!("cass")
        .args(["search", "rebuild_test_initial", "--robot", "--data-dir"])
        .arg(&data_dir)
        .env("HOME", home)
        .output()
        .expect("search command");

    assert!(output.status.success());
    let json: serde_json::Value = serde_json::from_slice(&output.stdout).expect("valid JSON");
    let hits = json.get("hits").and_then(|h| h.as_array()).expect("hits");
    assert!(!hits.is_empty(), "Content should still be searchable");
}

/// Test: JSON output mode (--json) for index command
#[test]
fn index_json_output_mode() {
    let tmp = tempfile::TempDir::new().unwrap();
    let home = tmp.path();
    let codex_home = home.join(".codex");
    let data_dir = home.join("cass_data");
    fs::create_dir_all(&data_dir).unwrap();

    let _guard_home = EnvGuard::set("HOME", home.to_string_lossy());
    let _guard_codex = EnvGuard::set("CODEX_HOME", codex_home.to_string_lossy());

    // Create fixture
    make_codex_session(
        &codex_home,
        "2024/11/20",
        "rollout-json.jsonl",
        "json_output_test",
        1732118400000,
    );

    // Index with --json
    let output = cargo_bin_cmd!("cass")
        .args(["index", "--full", "--json", "--data-dir"])
        .arg(&data_dir)
        .env("CODEX_HOME", &codex_home)
        .env("HOME", home)
        .output()
        .expect("index command");

    assert!(output.status.success());

    // Debug: print actual output
    eprintln!(
        "Index JSON output: {}",
        String::from_utf8_lossy(&output.stdout)
    );

    // Verify JSON output structure - index --json outputs various fields
    let json: serde_json::Value =
        serde_json::from_slice(&output.stdout).expect("valid JSON output");

    // Index JSON output should be a valid JSON object
    assert!(
        json.is_object(),
        "JSON output should be an object. Got: {}",
        json
    );
}

/// Test: Help text includes expected options
#[test]
fn index_help_includes_options() {
    let output = cargo_bin_cmd!("cass")
        .args(["index", "--help"])
        .output()
        .expect("help command");

    assert!(output.status.success());
    let stdout = String::from_utf8_lossy(&output.stdout);

    assert!(stdout.contains("--full"), "Help should mention --full");
    assert!(stdout.contains("--watch"), "Help should mention --watch");
    assert!(
        stdout.contains("--force-rebuild"),
        "Help should mention --force-rebuild"
    );
    assert!(
        stdout.contains("--semantic"),
        "Help should mention --semantic"
    );
    assert!(
        stdout.contains("--embedder"),
        "Help should mention --embedder"
    );
    assert!(
        stdout.contains("--data-dir"),
        "Help should mention --data-dir"
    );
}

/// Test: Search help includes expected options
#[test]
fn search_help_includes_options() {
    let output = cargo_bin_cmd!("cass")
        .args(["search", "--help"])
        .output()
        .expect("help command");

    assert!(output.status.success());
    let stdout = String::from_utf8_lossy(&output.stdout);

    assert!(stdout.contains("--robot"), "Help should mention --robot");
    assert!(stdout.contains("--limit"), "Help should mention --limit");
    assert!(stdout.contains("--agent"), "Help should mention --agent");
    assert!(
        stdout.contains("--aggregate"),
        "Help should mention --aggregate"
    );
}

/// Test: Search with wildcard query
#[test]
fn search_wildcard_query() {
    let tmp = tempfile::TempDir::new().unwrap();
    let home = tmp.path();
    let codex_home = home.join(".codex");
    let data_dir = home.join("cass_data");
    fs::create_dir_all(&data_dir).unwrap();

    let _guard_home = EnvGuard::set("HOME", home.to_string_lossy());
    let _guard_codex = EnvGuard::set("CODEX_HOME", codex_home.to_string_lossy());

    // Create fixture with unique prefix
    make_codex_session(
        &codex_home,
        "2024/11/20",
        "rollout-wild.jsonl",
        "wildcardtest_unique_suffix",
        1732118400000,
    );

    // Index
    cargo_bin_cmd!("cass")
        .args(["index", "--full", "--data-dir"])
        .arg(&data_dir)
        .env("CODEX_HOME", &codex_home)
        .env("HOME", home)
        .assert()
        .success();

    // Search with wildcard prefix
    let output = cargo_bin_cmd!("cass")
        .args(["search", "wildcardtest*", "--robot", "--data-dir"])
        .arg(&data_dir)
        .env("HOME", home)
        .output()
        .expect("search command");

    assert!(output.status.success());
    let json: serde_json::Value = serde_json::from_slice(&output.stdout).expect("valid JSON");
    let hits = json.get("hits").and_then(|h| h.as_array()).expect("hits");

    assert!(
        !hits.is_empty(),
        "Wildcard prefix search should find results"
    );
}

/// Test: Trace logging works when enabled
#[test]
fn trace_logging_to_file() {
    let tmp = tempfile::TempDir::new().unwrap();
    let home = tmp.path();
    let codex_home = home.join(".codex");
    let data_dir = home.join("cass_data");
    let trace_dir = home.join("traces");
    fs::create_dir_all(&data_dir).unwrap();
    fs::create_dir_all(&trace_dir).unwrap();

    let _guard_home = EnvGuard::set("HOME", home.to_string_lossy());
    let _guard_codex = EnvGuard::set("CODEX_HOME", codex_home.to_string_lossy());
    let _guard_trace = EnvGuard::set("CASS_TRACE_DIR", trace_dir.to_string_lossy());

    // Create fixture
    make_codex_session(
        &codex_home,
        "2024/11/20",
        "rollout-trace.jsonl",
        "trace_test_content",
        1732118400000,
    );

    // Index with tracing enabled
    cargo_bin_cmd!("cass")
        .args(["index", "--full", "--data-dir"])
        .arg(&data_dir)
        .env("CODEX_HOME", &codex_home)
        .env("HOME", home)
        .env("CASS_TRACE_DIR", &trace_dir)
        .assert()
        .success();

    // Note: Trace file creation depends on tracing-appender setup in the binary
    // This test verifies the env var is recognized without crashing
}

/// Test: Empty query returns recent results
#[test]
fn empty_query_returns_recent() {
    let tmp = tempfile::TempDir::new().unwrap();
    let home = tmp.path();
    let codex_home = home.join(".codex");
    let data_dir = home.join("cass_data");
    fs::create_dir_all(&data_dir).unwrap();

    let _guard_home = EnvGuard::set("HOME", home.to_string_lossy());
    let _guard_codex = EnvGuard::set("CODEX_HOME", codex_home.to_string_lossy());

    // Create fixture
    make_codex_session(
        &codex_home,
        "2024/11/20",
        "rollout-recent.jsonl",
        "recent_results_test",
        1732118400000,
    );

    // Index
    cargo_bin_cmd!("cass")
        .args(["index", "--full", "--data-dir"])
        .arg(&data_dir)
        .env("CODEX_HOME", &codex_home)
        .env("HOME", home)
        .assert()
        .success();

    // Search with empty query (should show recent)
    let output = cargo_bin_cmd!("cass")
        .args(["search", "", "--robot", "--data-dir"])
        .arg(&data_dir)
        .env("HOME", home)
        .output()
        .expect("search command");

    // Empty query might return recent or error - both are valid behaviors
    // Just verify it doesn't crash
    assert!(
        output.status.success() || output.status.code() == Some(0),
        "Empty query should not crash"
    );
}
