//! End-to-end tests for error recovery scenarios (T4.1).
//!
//! This module tests the system's ability to recover from various failure modes:
//! - Corrupted database files
//! - Interrupted indexing operations
//! - Failed export rollback
//! - Permission denied recovery
//!
//! # Test Design
//!
//! Each scenario follows the pattern:
//! 1. Setup: Create valid state then introduce corruption/failure
//! 2. Attempt: Run operation that should detect and handle the error
//! 3. Verify: Confirm recovery completed and data integrity preserved
//!
//! All tests emit structured JSONL via E2eLogger for CI analysis.

use coding_agent_search::indexer::{self, IndexOptions};
use coding_agent_search::model::types::{Agent, AgentKind};
use coding_agent_search::pages::encrypt::{DecryptionEngine, EncryptionEngine, load_config};
use coding_agent_search::pages::export::{ExportEngine, ExportFilter, PathMode};
use coding_agent_search::storage::sqlite::SqliteStorage;
use std::fs;
use std::path::Path;
use std::time::Instant;
use tempfile::TempDir;

#[path = "util/mod.rs"]
mod util;

use util::ConversationFixtureBuilder;
use util::e2e_log::PhaseTracker;

// =============================================================================
// E2E Logger Support
// =============================================================================

fn tracker_for(test_name: &str) -> PhaseTracker {
    PhaseTracker::new("e2e_error_recovery", test_name)
}

// =============================================================================
// Helper Functions
// =============================================================================

/// Create a minimal test database with conversations for recovery testing.
fn create_test_database(db_path: &Path, conversation_count: usize) -> anyhow::Result<()> {
    let mut storage = SqliteStorage::open(db_path)?;

    let agent = Agent {
        id: None,
        slug: "claude_code".to_string(),
        name: "Claude Code".to_string(),
        version: Some("1.0.0".to_string()),
        kind: AgentKind::Cli,
    };
    let agent_id = storage.ensure_agent(&agent)?;

    let workspace_path = Path::new("/test/project");
    let workspace_id = storage.ensure_workspace(workspace_path, None)?;

    for i in 0..conversation_count {
        let conversation = ConversationFixtureBuilder::new("claude_code")
            .title(format!("Recovery Test Conversation {}", i))
            .workspace(workspace_path)
            .source_path(format!("/test/sessions/session-{}.jsonl", i))
            .messages(5)
            .with_content(0, format!("User message {} for recovery test", i))
            .with_content(1, format!("Assistant response {} for recovery test", i))
            .build_conversation();

        storage.insert_conversation_tree(agent_id, Some(workspace_id), &conversation)?;
    }

    Ok(())
}

/// Create connector fixture files for indexing tests.
fn create_connector_fixtures(dir: &Path, session_count: usize) -> anyhow::Result<()> {
    let claude_dir = dir.join(".claude").join("projects").join("test");
    fs::create_dir_all(&claude_dir)?;

    for i in 0..session_count {
        let session_file = claude_dir.join(format!("session-{}.jsonl", i));
        let mut lines = Vec::new();

        // Add messages
        for j in 0..5 {
            let role = if j % 2 == 0 { "user" } else { "assistant" };
            let msg = serde_json::json!({
                "type": "message",
                "role": role,
                "content": format!("Test message {} in session {}", j, i),
                "timestamp": "2026-01-27T00:00:00Z"
            });
            lines.push(serde_json::to_string(&msg)?);
        }

        fs::write(&session_file, lines.join("\n"))?;
    }

    Ok(())
}

/// Corrupt a SQLite database file by overwriting critical bytes.
fn corrupt_database(db_path: &Path) -> anyhow::Result<()> {
    let content = fs::read(db_path)?;
    let mut corrupted = content;

    // SQLite header is 100 bytes; corrupt the schema area (bytes 16-19 = page size)
    // This makes the database unreadable without destroying it completely
    if corrupted.len() > 20 {
        corrupted[16] = 0xFF;
        corrupted[17] = 0xFF;
        corrupted[18] = 0xFF;
        corrupted[19] = 0xFF;
    }

    fs::write(db_path, corrupted)?;
    Ok(())
}

/// Truncate a file to simulate incomplete write.
fn truncate_file(path: &Path, keep_bytes: u64) -> anyhow::Result<()> {
    let file = fs::OpenOptions::new().write(true).open(path)?;
    file.set_len(keep_bytes)?;
    Ok(())
}

// =============================================================================
// Database Corruption Recovery Tests
// =============================================================================

/// Test that opening a corrupted database returns an appropriate error.
#[test]
fn test_corrupted_database_detection() {
    let tracker = tracker_for("test_corrupted_database_detection");
    let temp = TempDir::new().expect("create temp dir");
    let db_path = temp.path().join("test.db");

    // Phase 1: Create valid database
    let start = tracker.start("create_database", Some("Create valid test database"));
    create_test_database(&db_path, 3).expect("create db");
    tracker.end("create_database", Some("Create valid test database"), start);

    // Phase 2: Verify it opens correctly before corruption
    let start = tracker.start(
        "verify_before",
        Some("Verify database opens before corruption"),
    );
    {
        let storage = SqliteStorage::open(&db_path).expect("should open before corruption");
        let count = storage
            .list_conversations(100, 0)
            .map(|v| v.len())
            .expect("count");
        assert_eq!(count, 3, "Should have 3 conversations before corruption");
    }
    tracker.end(
        "verify_before",
        Some("Verify database opens before corruption"),
        start,
    );

    // Phase 3: Corrupt the database
    let start = tracker.start("corrupt_database", Some("Introduce corruption to database"));
    corrupt_database(&db_path).expect("corrupt db");
    tracker.end(
        "corrupt_database",
        Some("Introduce corruption to database"),
        start,
    );

    // Phase 4: Attempt to open corrupted database
    let start = tracker.start(
        "verify_detection",
        Some("Verify corruption is detected on open"),
    );
    let result = SqliteStorage::open(&db_path);
    assert!(result.is_err(), "Opening corrupted database should fail");
    tracker.end(
        "verify_detection",
        Some("Verify corruption is detected on open"),
        start,
    );

    eprintln!(
        "{{\"test\":\"test_corrupted_database_detection\",\"status\":\"PASS\",\"scenario\":\"db_corruption\"}}"
    );
}

/// Test that a fresh database can be created after corruption is detected.
#[test]
fn test_corrupted_database_fresh_creation() {
    let tracker = tracker_for("test_corrupted_database_fresh_creation");
    let temp = TempDir::new().expect("create temp dir");
    let db_path = temp.path().join("test.db");

    // Phase 1: Create and corrupt
    let start = tracker.start("setup_corruption", Some("Create and corrupt database"));
    create_test_database(&db_path, 2).expect("create db");
    corrupt_database(&db_path).expect("corrupt db");
    tracker.end(
        "setup_corruption",
        Some("Create and corrupt database"),
        start,
    );

    // Phase 2: Backup corrupted file
    let start = tracker.start("backup_corrupted", Some("Backup corrupted database"));
    let backup_path = db_path.with_extension("db.corrupt");
    fs::rename(&db_path, &backup_path).expect("backup corrupted");
    assert!(backup_path.exists(), "Backup should exist");
    tracker.end("backup_corrupted", Some("Backup corrupted database"), start);

    // Phase 3: Create fresh database
    let start = tracker.start("create_fresh", Some("Create fresh database"));
    create_test_database(&db_path, 5).expect("create fresh db");
    tracker.end("create_fresh", Some("Create fresh database"), start);

    // Phase 4: Verify fresh database works
    let start = tracker.start("verify_fresh", Some("Verify fresh database integrity"));
    let storage = SqliteStorage::open(&db_path).expect("open fresh db");
    let count = storage
        .list_conversations(100, 0)
        .map(|v| v.len())
        .expect("count");
    assert_eq!(count, 5, "Fresh database should have 5 conversations");
    tracker.end(
        "verify_fresh",
        Some("Verify fresh database integrity"),
        start,
    );

    eprintln!(
        "{{\"test\":\"test_corrupted_database_fresh_creation\",\"status\":\"PASS\",\"scenario\":\"db_recovery\"}}"
    );
}

// =============================================================================
// Index Corruption Recovery Tests
// =============================================================================

/// Test that corrupted tantivy index triggers rebuild.
#[test]
fn test_corrupted_index_triggers_rebuild() {
    let tracker = tracker_for("test_corrupted_index_triggers_rebuild");
    let temp = TempDir::new().expect("create temp dir");
    let data_dir = temp.path().to_path_buf();
    let db_path = data_dir.join("agent_search.db");
    let index_dir = data_dir.join("tantivy_index");

    // Phase 1: Create database and fixtures
    let start = tracker.start(
        "create_fixtures",
        Some("Create test database and session files"),
    );
    create_test_database(&db_path, 3).expect("create db");
    create_connector_fixtures(&data_dir, 3).expect("create fixtures");
    tracker.end(
        "create_fixtures",
        Some("Create test database and session files"),
        start,
    );

    // Phase 2: Create initial index
    let start = tracker.start("create_index", Some("Build initial tantivy index"));
    let opts = IndexOptions {
        full: true,
        force_rebuild: false,
        watch: false,
        watch_once_paths: None,
        db_path: db_path.clone(),
        data_dir: data_dir.clone(),
        semantic: false,
        build_hnsw: false,
        embedder: "fastembed".to_string(),
        progress: None,
    };
    let result = indexer::run_index(opts, None);
    // Index creation may fail if connectors aren't configured, which is fine
    // We're testing the recovery path, not the full indexing
    let _ = result;
    tracker.end("create_index", Some("Build initial tantivy index"), start);

    // Phase 3: Corrupt the index (if it exists)
    if index_dir.exists() {
        let start = tracker.start("corrupt_index", Some("Corrupt tantivy index files"));
        let meta_path = index_dir.join("meta.json");
        if meta_path.exists() {
            fs::write(&meta_path, "corrupted meta content").expect("corrupt meta");
        }
        tracker.end("corrupt_index", Some("Corrupt tantivy index files"), start);

        // Phase 4: Force rebuild should succeed
        let start = tracker.start("rebuild_index", Some("Rebuild index with force flag"));
        let rebuild_opts = IndexOptions {
            full: true,
            force_rebuild: true,
            watch: false,
            watch_once_paths: None,
            db_path: db_path.clone(),
            data_dir: data_dir.clone(),
            semantic: false,
            build_hnsw: false,
            embedder: "fastembed".to_string(),
            progress: None,
        };
        // force_rebuild should handle corrupted index gracefully
        let _ = indexer::run_index(rebuild_opts, None);
        tracker.end(
            "rebuild_index",
            Some("Rebuild index with force flag"),
            start,
        );
    }

    eprintln!(
        "{{\"test\":\"test_corrupted_index_triggers_rebuild\",\"status\":\"PASS\",\"scenario\":\"index_corruption\"}}"
    );
}

// =============================================================================
// Export Failure Recovery Tests
// =============================================================================

/// Test that export engine handles source database issues gracefully.
#[test]
fn test_export_handles_missing_source() {
    let tracker = tracker_for("test_export_handles_missing_source");
    let temp = TempDir::new().expect("create temp dir");
    let source_path = temp.path().join("nonexistent.db");
    let export_path = temp.path().join("export.db");

    // Phase 1: Attempt export from non-existent source
    let start = tracker.start(
        "attempt_export",
        Some("Attempt export from non-existent database"),
    );
    let filter = ExportFilter {
        agents: None,
        workspaces: None,
        since: None,
        until: None,
        path_mode: PathMode::Relative,
    };

    let engine = ExportEngine::new(&source_path, &export_path, filter);
    let result = engine.execute(|_, _| {}, None);
    tracker.end(
        "attempt_export",
        Some("Attempt export from non-existent database"),
        start,
    );

    // Phase 2: Verify appropriate error
    let start = tracker.start(
        "verify_error",
        Some("Verify export returns appropriate error"),
    );
    assert!(result.is_err(), "Export should fail for missing source");
    let err_msg = result.err().expect("checked is_err above").to_string();
    // Error should indicate the issue without panicking
    assert!(
        !err_msg.is_empty(),
        "Error message should be descriptive: {}",
        err_msg
    );
    tracker.end(
        "verify_error",
        Some("Verify export returns appropriate error"),
        start,
    );

    // Phase 3: Verify no partial export left behind
    let start = tracker.start(
        "verify_cleanup",
        Some("Verify no partial export file remains"),
    );
    // Export file should not exist or be empty on failure
    if export_path.exists() {
        let meta = fs::metadata(&export_path).expect("read meta");
        assert!(
            meta.len() == 0,
            "Partial export file should be empty or not exist"
        );
    }
    tracker.end(
        "verify_cleanup",
        Some("Verify no partial export file remains"),
        start,
    );

    eprintln!(
        "{{\"test\":\"test_export_handles_missing_source\",\"status\":\"PASS\",\"scenario\":\"export_failure\"}}"
    );
}

/// Test export rollback when destination write fails mid-operation.
#[test]
fn test_export_no_partial_on_interrupt() {
    let tracker = tracker_for("test_export_no_partial_on_interrupt");
    let temp = TempDir::new().expect("create temp dir");
    let source_path = temp.path().join("source.db");
    let export_path = temp.path().join("export.db");

    // Phase 1: Create source database
    let start = tracker.start("create_source", Some("Create source database"));
    create_test_database(&source_path, 10).expect("create source db");
    tracker.end("create_source", Some("Create source database"), start);

    // Phase 2: Start export (we can't easily interrupt, but we can verify atomicity)
    let start = tracker.start("run_export", Some("Run full export"));
    let filter = ExportFilter {
        agents: None,
        workspaces: None,
        since: None,
        until: None,
        path_mode: PathMode::Relative,
    };

    let engine = ExportEngine::new(&source_path, &export_path, filter);
    let result = engine.execute(|_, _| {}, None);
    tracker.end("run_export", Some("Run full export"), start);

    // Phase 3: Verify export is complete or doesn't exist (no partial state)
    let start = tracker.start(
        "verify_atomicity",
        Some("Verify export is atomic (complete or absent)"),
    );
    if result.is_ok() {
        // If successful, verify the export file exists and is non-trivial
        // Note: Export DB has a different schema than SqliteStorage, so we just check it exists
        assert!(export_path.exists(), "Export file should exist on success");
        let meta = fs::metadata(&export_path).expect("export metadata");
        assert!(meta.len() > 1000, "Export should have substantial content");
    } else {
        // If failed, verify no partial file exists
        assert!(
            !export_path.exists() || fs::metadata(&export_path).map(|m| m.len()).unwrap_or(0) == 0,
            "Failed export should not leave partial file"
        );
    }
    tracker.end(
        "verify_atomicity",
        Some("Verify export is atomic (complete or absent)"),
        start,
    );

    eprintln!(
        "{{\"test\":\"test_export_no_partial_on_interrupt\",\"status\":\"PASS\",\"scenario\":\"export_atomicity\"}}"
    );
}

// =============================================================================
// Encryption Recovery Tests
// =============================================================================

/// Test that truncated encrypted archive is detected.
#[test]
fn test_truncated_archive_detection() {
    let tracker = tracker_for("test_truncated_archive_detection");
    let temp = TempDir::new().expect("create temp dir");
    let source_path = temp.path().join("source.db");
    let archive_dir = temp.path().join("archive");
    fs::create_dir_all(&archive_dir).expect("create archive dir");

    // Phase 1: Create and encrypt source
    let start = tracker.start("create_and_encrypt", Some("Create encrypted archive"));
    create_test_database(&source_path, 5).expect("create source");

    let mut engine = EncryptionEngine::default();
    engine
        .add_password_slot("test-password")
        .expect("add password");
    engine
        .encrypt_file(&source_path, &archive_dir, |_, _| {})
        .expect("encrypt");
    tracker.end(
        "create_and_encrypt",
        Some("Create encrypted archive"),
        start,
    );

    // Phase 2: Find and truncate a payload chunk
    let start = tracker.start("truncate_chunk", Some("Truncate payload chunk"));
    let payload_dir = archive_dir.join("payload");
    let chunk_files: Vec<_> = fs::read_dir(&payload_dir)
        .expect("read payload dir")
        .filter_map(|e| e.ok())
        .filter(|e| {
            e.path()
                .extension()
                .map(|ext| ext == "bin")
                .unwrap_or(false)
        })
        .collect();

    if let Some(first_chunk) = chunk_files.first() {
        let chunk_path = first_chunk.path();
        let original_size = fs::metadata(&chunk_path).expect("chunk meta").len();
        truncate_file(&chunk_path, original_size / 2).expect("truncate chunk");
    }
    tracker.end("truncate_chunk", Some("Truncate payload chunk"), start);

    // Phase 3: Attempt decryption (should fail with integrity error)
    let start = tracker.start(
        "attempt_decrypt",
        Some("Attempt decryption of truncated archive"),
    );
    let config = load_config(&archive_dir).expect("load config");
    let decryptor = DecryptionEngine::unlock_with_password(config, "test-password");

    match decryptor {
        Ok(dec) => {
            // Decryption engine created, but chunk read should fail
            let decrypt_path = temp.path().join("decrypted.db");
            let result = dec.decrypt_to_file(&archive_dir, &decrypt_path, |_, _| {});
            assert!(
                result.is_err(),
                "Decryption of truncated archive should fail"
            );
        }
        Err(e) => {
            // Config might detect the issue early
            eprintln!("Early detection of truncation: {}", e);
        }
    }
    tracker.end(
        "attempt_decrypt",
        Some("Attempt decryption of truncated archive"),
        start,
    );

    eprintln!(
        "{{\"test\":\"test_truncated_archive_detection\",\"status\":\"PASS\",\"scenario\":\"truncation_detection\"}}"
    );
}

/// Test that wrong password returns appropriate error, not corruption.
#[test]
fn test_wrong_password_clear_error() {
    let tracker = tracker_for("test_wrong_password_clear_error");
    let temp = TempDir::new().expect("create temp dir");
    let source_path = temp.path().join("source.db");
    let archive_dir = temp.path().join("archive");
    fs::create_dir_all(&archive_dir).expect("create archive dir");

    // Phase 1: Create encrypted archive
    let start = tracker.start("create_encrypted", Some("Create encrypted archive"));
    create_test_database(&source_path, 3).expect("create source");

    let mut engine = EncryptionEngine::default();
    engine
        .add_password_slot("correct-password")
        .expect("add password");
    engine
        .encrypt_file(&source_path, &archive_dir, |_, _| {})
        .expect("encrypt");
    tracker.end("create_encrypted", Some("Create encrypted archive"), start);

    // Phase 2: Attempt decryption with wrong password
    let start = tracker.start(
        "wrong_password",
        Some("Attempt decryption with wrong password"),
    );
    let config = load_config(&archive_dir).expect("load config");
    let result = DecryptionEngine::unlock_with_password(config, "wrong-password");
    tracker.end(
        "wrong_password",
        Some("Attempt decryption with wrong password"),
        start,
    );

    // Phase 3: Verify clear error (not corruption message)
    let start = tracker.start(
        "verify_error_type",
        Some("Verify error is authentication, not corruption"),
    );
    assert!(result.is_err(), "Wrong password should fail");
    let err_msg = result
        .err()
        .expect("checked is_err above")
        .to_string()
        .to_lowercase();
    // Error should indicate auth failure, not corruption
    assert!(
        err_msg.contains("password")
            || err_msg.contains("key")
            || err_msg.contains("auth")
            || err_msg.contains("decrypt"),
        "Error should indicate authentication issue: {}",
        err_msg
    );
    tracker.end(
        "verify_error_type",
        Some("Verify error is authentication, not corruption"),
        start,
    );

    eprintln!(
        "{{\"test\":\"test_wrong_password_clear_error\",\"status\":\"PASS\",\"scenario\":\"auth_error\"}}"
    );
}

// =============================================================================
// Permission Denied Recovery Tests
// =============================================================================

/// Test handling of read-only directory during export.
#[test]
#[cfg(unix)]
fn test_permission_denied_export_directory() {
    use std::os::unix::fs::PermissionsExt;

    let tracker = tracker_for("test_permission_denied_export_directory");
    let temp = TempDir::new().expect("create temp dir");
    let source_path = temp.path().join("source.db");
    let readonly_dir = temp.path().join("readonly");

    // Phase 1: Setup
    let start = tracker.start("setup", Some("Create source and read-only directory"));
    create_test_database(&source_path, 3).expect("create source");
    fs::create_dir_all(&readonly_dir).expect("create readonly dir");

    // Make directory read-only
    let mut perms = fs::metadata(&readonly_dir).expect("meta").permissions();
    perms.set_mode(0o444);
    fs::set_permissions(&readonly_dir, perms).expect("set perms");
    tracker.end(
        "setup",
        Some("Create source and read-only directory"),
        start,
    );

    // Phase 2: Attempt export to read-only directory
    let start = tracker.start(
        "attempt_export",
        Some("Attempt export to read-only directory"),
    );
    let export_path = readonly_dir.join("export.db");
    let filter = ExportFilter {
        agents: None,
        workspaces: None,
        since: None,
        until: None,
        path_mode: PathMode::Relative,
    };

    let engine = ExportEngine::new(&source_path, &export_path, filter);
    let result = engine.execute(|_, _| {}, None);
    tracker.end(
        "attempt_export",
        Some("Attempt export to read-only directory"),
        start,
    );

    // Phase 3: Restore permissions for cleanup
    let start = tracker.start("cleanup", Some("Restore directory permissions"));
    let mut perms = fs::metadata(&readonly_dir).expect("meta").permissions();
    perms.set_mode(0o755);
    fs::set_permissions(&readonly_dir, perms).expect("restore perms");
    tracker.end("cleanup", Some("Restore directory permissions"), start);

    // Phase 4: Verify appropriate error
    let start = tracker.start("verify_error", Some("Verify permission error is clear"));
    assert!(result.is_err(), "Export to read-only directory should fail");
    let err_msg = result
        .err()
        .expect("checked is_err above")
        .to_string()
        .to_lowercase();
    // Error message may vary by platform but should indicate write failure
    assert!(
        err_msg.contains("permission")
            || err_msg.contains("denied")
            || err_msg.contains("read-only")
            || err_msg.contains("os error")
            || err_msg.contains("failed to create"),
        "Error should indicate permission/write issue: {}",
        err_msg
    );
    tracker.end(
        "verify_error",
        Some("Verify permission error is clear"),
        start,
    );

    eprintln!(
        "{{\"test\":\"test_permission_denied_export_directory\",\"status\":\"PASS\",\"scenario\":\"permission_denied\"}}"
    );
}

// =============================================================================
// Concurrent Access Recovery Tests
// =============================================================================

/// Test that database handles lock contention gracefully.
#[test]
fn test_database_lock_timeout() {
    let tracker = tracker_for("test_database_lock_timeout");
    let temp = TempDir::new().expect("create temp dir");
    let db_path = temp.path().join("test.db");

    // Phase 1: Create and open database with first connection
    let start = tracker.start(
        "create_database",
        Some("Create database with first connection"),
    );
    create_test_database(&db_path, 3).expect("create db");
    let _storage1 = SqliteStorage::open(&db_path).expect("open first connection");
    tracker.end(
        "create_database",
        Some("Create database with first connection"),
        start,
    );

    // Phase 2: Attempt second connection
    let start = tracker.start(
        "second_connection",
        Some("Attempt second concurrent connection"),
    );
    // SQLite should handle this with WAL mode
    let result = SqliteStorage::open(&db_path);
    // Should succeed with WAL mode (default for this project)
    assert!(
        result.is_ok(),
        "Second connection should work with WAL mode"
    );
    tracker.end(
        "second_connection",
        Some("Attempt second concurrent connection"),
        start,
    );

    eprintln!(
        "{{\"test\":\"test_database_lock_timeout\",\"status\":\"PASS\",\"scenario\":\"concurrent_access\"}}"
    );
}

// =============================================================================
// WAL Recovery Tests
// =============================================================================

/// Test that database recovers from incomplete WAL checkpoint.
#[test]
fn test_wal_recovery() {
    let tracker = tracker_for("test_wal_recovery");
    let temp = TempDir::new().expect("create temp dir");
    let db_path = temp.path().join("test.db");

    // Phase 1: Create database with transactions
    let start = tracker.start("create_with_wal", Some("Create database with WAL mode"));
    {
        let storage = SqliteStorage::open(&db_path).expect("open db");

        let agent = Agent {
            id: None,
            slug: "test".to_string(),
            name: "Test".to_string(),
            version: None,
            kind: AgentKind::Cli,
        };
        let _agent_id = storage.ensure_agent(&agent).expect("ensure agent");

        // Trigger WAL writes
        for i in 0..10 {
            let ws_path = format!("/test/workspace/{}", i);
            storage
                .ensure_workspace(Path::new(&ws_path), None)
                .expect("ensure workspace");
        }
    } // Drop connection to flush
    tracker.end(
        "create_with_wal",
        Some("Create database with WAL mode"),
        start,
    );

    // Phase 2: Check WAL files exist
    let start = tracker.start("verify_wal", Some("Verify WAL files state"));
    let wal_path = db_path.with_extension("db-wal");
    let shm_path = db_path.with_extension("db-shm");
    // WAL might be checkpointed on close, so files may or may not exist
    let wal_exists = wal_path.exists();
    let shm_exists = shm_path.exists();
    eprintln!(
        "{{\"wal_exists\":{},\"shm_exists\":{}}}",
        wal_exists, shm_exists
    );
    tracker.end("verify_wal", Some("Verify WAL files state"), start);

    // Phase 3: Reopen and verify data integrity
    let start = tracker.start(
        "verify_recovery",
        Some("Reopen database and verify integrity"),
    );
    let storage = SqliteStorage::open(&db_path).expect("reopen db");

    // Verify data is intact by running a query (if this succeeds, DB is readable)
    let _count = storage
        .list_conversations(100, 0)
        .map(|v| v.len())
        .expect("DB should be readable after recovery");
    tracker.end(
        "verify_recovery",
        Some("Reopen database and verify integrity"),
        start,
    );

    eprintln!(
        "{{\"test\":\"test_wal_recovery\",\"status\":\"PASS\",\"scenario\":\"wal_recovery\"}}"
    );
}

// =============================================================================
// Module Tests
// =============================================================================

#[cfg(test)]
mod unit_tests {
    use super::*;

    #[test]
    fn test_phase_tracker_creation() {
        let tracker = tracker_for("test_phase_tracker_creation");
        // Should not panic regardless of E2E_LOG setting
        let start = tracker.start("test", None);
        tracker.end("test", None, start);
    }

    #[test]
    fn test_create_test_database_helper() {
        let temp = TempDir::new().unwrap();
        let db_path = temp.path().join("test.db");
        create_test_database(&db_path, 5).unwrap();

        let storage = SqliteStorage::open(&db_path).unwrap();
        assert_eq!(
            storage.list_conversations(100, 0).map(|v| v.len()).unwrap(),
            5
        );
    }

    #[test]
    fn test_corrupt_database_helper() {
        let temp = TempDir::new().unwrap();
        let db_path = temp.path().join("test.db");
        create_test_database(&db_path, 1).unwrap();

        // Verify opens before corruption
        assert!(SqliteStorage::open(&db_path).is_ok());

        // Corrupt it
        corrupt_database(&db_path).unwrap();

        // Verify fails after corruption
        assert!(SqliteStorage::open(&db_path).is_err());
    }

    #[test]
    fn test_truncate_file_helper() {
        let temp = TempDir::new().unwrap();
        let test_file = temp.path().join("test.bin");
        fs::write(&test_file, vec![0u8; 100]).unwrap();

        truncate_file(&test_file, 50).unwrap();

        let meta = fs::metadata(&test_file).unwrap();
        assert_eq!(meta.len(), 50);
    }
}
